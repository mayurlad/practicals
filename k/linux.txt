ld-linux-x86-64.so.2

command

g++ -fopenmp -o output_name code.cpp && ./output_name

-----

wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo dpkg -i google-chrome-stable_current_amd64.deb
google-chrome

-----------





HPC

#include <iostream>
#include <queue>
#include <omp.h>
using namespace std;

struct Node {
    int val;
    Node *left, *right;
    Node(int v) : val(v), left(nullptr), right(nullptr) {}
};

void parallelBFS(Node* root) {
    queue<Node*> q;
    q.push(root);
    while (!q.empty()) {
        int n = q.size();
        #pragma omp parallel for
        for (int i = 0; i < n; ++i) {
            Node* curr;
            #pragma omp critical
            {
                curr = q.front(); q.pop();
            }
            cout << curr->val << " ";
            if (curr->left) {
                #pragma omp critical
                q.push(curr->left);
            }
            if (curr->right) {
                #pragma omp critical
                q.push(curr->right);
            }
        }
    }
}

void parallelDFS(Node* root) {
    if (!root) return;
    #pragma omp parallel sections
    {
        #pragma omp section
        parallelDFS(root->left);
        #pragma omp section
        parallelDFS(root->right);
    }
    #pragma omp critical
    cout << root->val << " ";
}

int main() {
    Node* root = new Node(1);
    root->left = new Node(2);
    root->right = new Node(3);
    root->left->left = new Node(4);
    root->left->right = new Node(5);
    root->right->left = new Node(6);
    root->right->right = new Node(7);

    cout << "Parallel BFS: ";
    parallelBFS(root);
    cout << "\nParallel DFS: ";
    parallelDFS(root);
    cout << endl;
}

-----


hpc1

#include <iostream>
#include <vector>
#include <queue>
#include <omp.h>

using namespace std;

void bfs(const vector<vector<int>>& tree) {
    vector<bool> visited(tree.size(), false);
    queue<int> q;
    q.push(0);
    visited[0] = true;

    while (!q.empty()) {
        int levelSize = q.size();

        #pragma omp parallel for
        for (int i = 0; i < levelSize; ++i) {
            int node = q.front();
            q.pop();
            cout << node << " ";

            for (int j = 0; j < tree[node].size(); ++j) {
                int child = tree[node][j];
                if (!visited[child]) {
                    #pragma omp critical
                    {
                        q.push(child);
                        visited[child] = true;
                    }
                }
            }
        }
    }
}

int main() {
    vector<vector<int>> tree = {{1, 2}, {3, 4}, {5, 6}, {}, {}, {}, {}};
    bfs(tree);
    return 0;
}


--

// preorder dfs

#include <iostream>
#include <vector>
#include <omp.h>

using namespace std;

void dfs(const vector<vector<int>>& tree, vector<bool>& visited, int node) {
    visited[node] = true;
    cout << node << " ";

    #pragma omp taskgroup
    for (int i = 0; i < tree[node].size(); ++i) {
        int child = tree[node][i];
        if (!visited[child]) {
            #pragma omp task
            dfs(tree, visited, child);
        }
    }
}

int main() {
    // Define a tree structure
    vector<vector<int>> tree = {{1, 2}, {3, 4}, {5, 6}, {}, {}, {}, {}};
    vector<bool> visited(tree.size(), false);

    #pragma omp parallel
    {
        #pragma omp single
        dfs(tree, visited, 0);
    }

    return 0;
}

----
hpc2

bubble

#include <iostream>
#include <vector>
#include <omp.h>

using namespace std;

int main() {
    vector<int> arr = {9, 5, 2, 7, 1, 8, 3, 6, 4, 0};
    int n = arr.size();

    for (int phase = 0; phase < n; ++phase) {
        #pragma omp parallel for
        for (int i = (phase % 2); i < n - 1; i += 2) {
            if (arr[i] > arr[i + 1])
                swap(arr[i], arr[i + 1]);
        }
    }

    // Print result
    for (int val : arr)
        cout << val << " ";
    cout << endl;

    return 0;
}


//g++ -fopenmp -O2 parallel_bubble_sort.cpp -o bubble_sort

---

merge

#include <iostream>
#include <vector>
#include <algorithm>
#include <omp.h>

using namespace std;

int main() {
    vector<int> data = {9, 5, 2, 7, 1, 8, 3, 6, 4, 0};

    // Split data into two halves and sort in parallel
    #pragma omp parallel sections
    {
        #pragma omp section
        sort(data.begin(), data.begin() + data.size() / 2);

        #pragma omp section
        sort(data.begin() + data.size() / 2, data.end());
    }

    // Merge the two sorted halves
    inplace_merge(data.begin(), data.begin() + data.size() / 2, data.end());

    // Print result
    for (int val : data)
        cout << val << " ";
    cout << endl;

    return 0;
}

// g++ -fopenmp -O2 small_parallel_sort.cpp -o small_sort

--------

hpc3
#include <iostream>
#include <omp.h>
#include <vector>
#include <limits>

using namespace std;

int main() {
    // Example array
    vector<int> data = {12, 7, 9, 3, 15, 21, 5, 18};

    // Variables for Min, Max, Sum
    int min_val = numeric_limits<int>::max();
    int max_val = numeric_limits<int>::min();
    int sum_val = 0;

    // Parallel reduction with OpenMP
    #pragma omp parallel
    {
        #pragma omp for reduction(min:min_val) reduction(max:max_val) reduction(+:sum_val)
        for (int i = 0; i < data.size(); i++) {
            min_val = min(min_val, data[i]);
            max_val = max(max_val, data[i]);
            sum_val += data[i];
        }
    }

    // Calculate average
    double average = static_cast<double>(sum_val) / data.size();

    // Output the results
    cout << "Min: " << min_val << endl;
    cout << "Max: " << max_val << endl;
    cout << "Sum: " << sum_val << endl;
    cout << "Average: " << average << endl;

    return 0;
}
--------

hpc4

#include <iostream>
#include <vector>
#include <omp.h>
#include <chrono>

using namespace std;  // Avoid redundancy of 'std::'

int main() {
    const size_t size = 100000000; // 100 million elements

    vector<double> A(size, 1.0); // initialize all elements to 1.0
    vector<double> B(size, 2.0); // initialize all elements to 2.0
    vector<double> C(size, 0.0); // result vector

    auto start = chrono::high_resolution_clock::now();

    // Parallel vector addition
    #pragma omp parallel for
    for (size_t i = 0; i < size; ++i) {
        C[i] = A[i] + B[i];
    }

    auto end = chrono::high_resolution_clock::now();
    chrono::duration<double> elapsed = end - start;

    cout << "Time taken for vector addition: " << elapsed.count() << " seconds\n";
    cout << "Sample result: C[0] = " << C[0] << ", C[" << size - 1 << "] = " << C[size - 1] << endl;

    return 0;
}


// To compile this code with OpenMP, use:

// bash
// Copy
// Edit
// g++ -fopenmp -O2 vector_addition.cpp -o vector_add
// ðŸ’¡ Notes
// #pragma omp parallel for tells OpenMP to divide the loop iterations among threads.

// You can control the number of threads using OMP_NUM_THREADS:

// bash
// Copy
// Edit
// export OMP_NUM_THREADS=4
// ./vector_add

--  --
#include <iostream>
#include <vector>
#include <omp.h>
#include <chrono>

using namespace std;

int main() {
    const size_t size = 100000000; // 100 million elements

    vector<double> A(size, 1.5); // initialize all elements to 1.5
    vector<double> B(size, 2.5); // initialize all elements to 2.5
    vector<double> C(size, 0.0); // result vector

    auto start = chrono::high_resolution_clock::now();

    // Parallel element-wise vector multiplication
    #pragma omp parallel for
    for (size_t i = 0; i < size; ++i) {
        C[i] = A[i] * B[i];
    }

    auto end = chrono::high_resolution_clock::now();
    chrono::duration<double> elapsed = end - start;

    cout << "Time taken for vector multiplication: " << elapsed.count() << " seconds\n";
    cout << "Sample result: C[0] = " << C[0] << ", C[" << size - 1 << "] = " << C[size - 1] << endl;

    return 0;
}


// g++ -fopenmp -O2 vector_multiplication.cpp -o vector_mul

--------------------------



---------------------------
LAB1

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from tqdm.notebook import tqdm
import warnings
warnings.filterwarnings("ignore")

boston = tf.keras.datasets.boston_housing

dir(boston)

boston_data = boston.load_data()

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(path='boston_housing.npz', test_split=0.2, seed=42)

x_train.shape, y_train.shape, x_test.shape, y_test.shape

scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))
y_test_scaled = scaler.transform(y_test.reshape(-1, 1))

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(13), name='input-layer'),
    tf.keras.layers.Dense(100, name='hidden-layer-2'),
    tf.keras.layers.BatchNormalization(name='hidden-layer-3'),
    tf.keras.layers.Dense(50, name='hidden-layer-4'),
    tf.keras.layers.Dense(1, name='output-layer')
])

tf.keras.utils.plot_model(model, show_shapes=True)

model.summary()

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

history = model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_test, y_test))

pd.DataFrame(history.history).plot(figsize=(10,7))
plt.title("Metrics graph")
plt.show()

y_pred = model.predict(x_test)

sns.regplot(x=y_test, y=y_pred)
plt.title("Regression Line for Predicted values")
plt.show()

def regression_metrics_display(y_test, y_pred):
  print(f"MAE is {metrics.mean_absolute_error(y_test, y_pred)}")
  print(f"MSE is {metrics.mean_squared_error(y_test,y_pred)}")
  print(f"R2 score is {metrics.r2_score(y_test, y_pred)}")

  regression_metrics_display(y_test, y_pred)



-------------

lab2 without hfembedding

import tensorflow as tf
from mlxtend.plotting import plot_confusion_matrix
from sklearn import metrics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from tqdm.notebook import tqdm
import warnings
warnings.filterwarnings("ignore")

vocab_size = 10000
max_len = 200
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)

x_train.shape, y_train.shape, x_test.shape, y_test.shape

x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)
x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)

x_train.shape, y_train.shape, x_test.shape, y_test.shape

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 128, input_length=max_len),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

tf.keras.utils.plot_model(model, show_shapes=True)

model.summary()

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test))

pd.DataFrame(history.history).plot(figsize=(10,7))
plt.title("Model Metrics")
plt.show()

loss, accuracy = model.evaluate(x_test, y_test)
print("Test Accuracy:", accuracy)

y_pred = model.predict(x_test)

y_pred

y_pred = y_pred.flatten()

y_pred

y_pred = (y_pred > 0.5).astype(int)

print(metrics.classification_report(y_test, y_pred))

cm = metrics.confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cm, class_names=['Negative', 'Positive'])
plt.title("Confusion Matrix")
plt.show()

---
lab2

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from mlxtend.plotting import plot_confusion_matrix
from sklearn import metrics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from tqdm.notebook import tqdm
import warnings
warnings.filterwarnings("ignore")

------


lab3

import tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_confusion_matrix
from sklearn import metrics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from tqdm.notebook import tqdm
import random
import warnings
warnings.filterwarnings("ignore")

(trainX, trainY), (testX, testY) = tf.keras.datasets.fashion_mnist.load_data()

trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
testX = testX.reshape((testX.shape[0], 28, 28, 1))

trainY_cat = tf.keras.utils.to_categorical(trainY)
testY_cat = tf.keras.utils.to_categorical(testY)

train_norm = trainX.astype('float32')
test_norm = testX.astype('float32')

train_norm = train_norm / 255.0
test_norm = test_norm / 255.0

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

               plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(trainX[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[trainY[i]])
plt.show()

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu', padding='same', name='conv-layer-1'),
    tf.keras.layers.AvgPool2D(pool_size=(2, 2), name='pooling-layer-1'),
    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', name='conv-layer-2'),
    tf.keras.layers.AvgPool2D(pool_size=(2, 2), name='pooling-layer-2'),
    tf.keras.layers.GlobalAveragePooling2D(name='pooling-layer-3'),
    tf.keras.layers.Dense(len(class_names), activation="softmax", name="output-layer")
])

model.compile(loss="categorical_crossentropy",
             optimizer="adam",
             metrics=["accuracy"])

history = model.fit(trainX, trainY_cat, epochs=10, validation_data=(testX, testY_cat))

tf.keras.utils.plot_model(model, show_shapes=True)

model.summary()

pd.DataFrame(history.history).plot(figsize=(10,7))
plt.title("Metrics Graph")
plt.show()

model.evaluate(testX, testY_cat)

predictions = model.predict(testX)

predictions = tf.argmax(predictions, axis=1)

y_test = tf.argmax(testY_cat, axis=1)

y_test = tf.Variable(y_test)

print(metrics.accuracy_score(y_test, predictions))

print(metrics.classification_report(y_test, predictions))

cm = metrics.confusion_matrix(y_test, predictions)
plot_confusion_matrix(cm, figsize=(10,7), class_names=class_names)
plt.title("Confusion Matrix")
plt.show()

images = []
labels = []
random_indices = random.sample(range(len(testX)), 10)
for idx in random_indices:
    images.append(testX[idx])
    labels.append(testY_cat[idx])
images = np.array(images)
labels = np.array(labels)

fig = plt.figure(figsize=(20, 8))
rows = 2
cols = 5
x = 1
for image, label in zip(images, labels):
    fig.add_subplot(rows, cols, x)
    prediction = model.predict(tf.expand_dims(image, axis=0))
    prediction = class_names[tf.argmax(prediction.flatten())]
    label = class_names[tf.argmax(label)]
    plt.title(f"Label: {label}, Prediction: {prediction}")
    plt.imshow(image/255.)
    plt.axis("off")
    x += 1

-------------------------------------------

lab4

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import datetime
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.metrics import r2_score


data = pd.read_csv('Google_Stock_Price_Train.csv',thousands=',')
data

ax1 = data.plot(x="Date", y=["Open", "High", "Low", "Close"], figsize=(10,7),title='Open, High, Low, Close Stock Prices of Google Stocks')
ax1.set_ylabel("Stock Price")

ax2 = data.plot(x="Date", y=["Volume"],  figsize=(10,7))
ax2.set_ylabel("Stock Volume")

data.isna().sum()

data[['Open','High','Low','Close','Volume']].plot(kind='box', layout=(1,5), subplots=True, sharex=False, sharey=False, figsize=(10,7),color='red')
plt.show()

data.hist(figsize=(10,7))
plt.show()

scaler = MinMaxScaler()
data_without_date = data.drop("Date", axis=1)
scaled_data = pd.DataFrame(scaler.fit_transform(data_without_date))

scaled_data.hist(figsize=(10,7))
plt.show()

plt.figure(figsize=(10,7))
sns.heatmap(data.drop("Date", axis=1).corr())
plt.show()

scaled_data = scaled_data.drop([0, 2, 3], axis=1)
scaled_data

def split_seq_multivariate(sequence, n_past, n_future):

    '''
    n_past ==> no of past observations
    n_future ==> no of future observations
    '''
    x = []
    y = []
    for window_start in range(len(sequence)):
        past_end = window_start + n_past
        future_end = past_end + n_future
        if future_end > len(sequence):
            break
        # slicing the past and future parts of the window (this indexing is for 2 features vala data only)
        past = sequence[window_start:past_end, :]
        future = sequence[past_end:future_end, -1]
        x.append(past)
        y.append(future)

    return np.array(x), np.array(y)

n_steps = 60

scaled_data = scaled_data.to_numpy()
scaled_data.shape

x, y = split_seq_multivariate(scaled_data, n_steps, 1)

x.shape, y.shape

y = y[:, 0]
y.shape

x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, random_state=42)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

model = Sequential()
model.add(LSTM(612, input_shape=(n_steps, 2)))
model.add(Dense(50, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(30, activation='relu'))
model.add(Dense(1))

model.summary()

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

history = model.fit(x_train, y_train, epochs=250, batch_size=32, verbose=2, validation_data=(x_test, y_test))

pd.DataFrame(history.history).plot(figsize=(10,7))

----------------------------------------------





