{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4eeb576",
   "metadata": {},
   "source": [
    "#### Madhur Jaripatke\n",
    "#### Roll No. 50\n",
    "#### BE A Computer\n",
    "#### RMDSSOE, Warje, Pune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa45e0",
   "metadata": {},
   "source": [
    "# Fashion MNIST Classification using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements a CNN to classify fashion items into 10 categories using the Fashion MNIST dataset.\n",
    "\n",
    "## Problem Statement:\n",
    "**Convolutional neural network (CNN):**\n",
    "Use MNIST Fashion Dataset and create a classifier to classify fashion clothing into \n",
    "categories. \n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "Fashion MNIST is a dataset of Zalando's article images consisting of:\n",
    "- 70,000 grayscale images (60,000 training, 10,000 testing)\n",
    "- 10 categories of clothing items\n",
    "- Image size: 28x28 pixels\n",
    "\n",
    "### Classes\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1d953",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We'll load the Fashion MNIST dataset and:\n",
    "1. Normalize pixel values to [0, 1]\n",
    "2. Reshape images to include channel dimension\n",
    "3. Convert labels to categorical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist():\n",
    "    \"\"\"Load and preprocess the Fashion MNIST dataset.\"\"\"\n",
    "    print(\"Loading Fashion MNIST dataset...\")\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    \n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    # Reshape images to include channel dimension\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = load_fashion_mnist()\n",
    "\n",
    "# Create validation split\n",
    "val_samples = 5000\n",
    "X_val = X_train[:val_samples]\n",
    "y_val = y_train[:val_samples]\n",
    "X_train = X_train[val_samples:]\n",
    "y_train = y_train[val_samples:]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Display some sample images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Class {np.argmax(y_train[i])}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e58ddd",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We'll create a CNN with:\n",
    "- Multiple convolutional blocks\n",
    "- Batch normalization for training stability\n",
    "- MaxPooling for spatial dimension reduction\n",
    "- Dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ea5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    \"\"\"Creates a CNN model for Fashion MNIST classification.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "model = create_cnn_model()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca90b8f",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We'll train with:\n",
    "- Early stopping\n",
    "- Learning rate reduction\n",
    "- Batch size of 64\n",
    "- Maximum 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc3b82",
   "metadata": {},
   "source": [
    "## Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the training and validation accuracy/loss curves.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e41501",
   "metadata": {},
   "source": [
    "## Model Evaluation and Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(model, X_test, y_test, num_samples=10):\n",
    "    \"\"\"Plot sample predictions with actual and predicted labels.\"\"\"\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "    predictions = model.predict(X_test[:num_samples])\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "        true_label = class_names[np.argmax(y_test[i])]\n",
    "        pred_label = class_names[np.argmax(predictions[i])]\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        plt.title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot sample predictions\n",
    "print(\"\\nGenerating sample predictions...\")\n",
    "plot_sample_predictions(model, X_test, y_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
