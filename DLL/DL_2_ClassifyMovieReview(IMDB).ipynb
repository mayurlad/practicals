{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b2f064",
   "metadata": {},
   "source": [
    "#### Madhur Jaripatke\n",
    "#### Roll No. 50\n",
    "#### BE A Computer\n",
    "#### RMDSSOE, Warje, Pune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01122bbf",
   "metadata": {},
   "source": [
    "# IMDB Movie Review Sentiment Classification\n",
    "\n",
    "This notebook implements a deep learning model for sentiment analysis using the IMDB movie reviews dataset.\n",
    "\n",
    "## Problem Statement\n",
    "**Classification using Deep neural network:**\n",
    "Binary classification using Deep Neural Networks Example: Classify movie reviews into \n",
    "positive\" reviews and \"negative\" reviews, just based on the text content of the reviews. \n",
    "Use IMDB dataset \n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews split evenly into 25,000 training and 25,000 test sets. The sentiment of reviews is binary:\n",
    "- 0: Negative\n",
    "- 1: Positive\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "We'll implement a hybrid neural network that combines:\n",
    "1. Word embeddings for text representation\n",
    "2. 1D CNN for feature extraction\n",
    "3. LSTM layers for sequential pattern learning\n",
    "4. Dense layers for final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e020cf3",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We'll load the IMDB dataset and preprocess it by:\n",
    "1. Limiting vocabulary to top 10,000 words\n",
    "2. Padding sequences to a maximum length of 200 words\n",
    "3. Creating validation split from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(num_words=10000, maxlen=200):\n",
    "    \"\"\"Load and preprocess the IMDB dataset.\"\"\"\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "    \n",
    "    # Pad sequences to ensure uniform length\n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = load_imdb_data()\n",
    "\n",
    "# Create validation split\n",
    "val_samples = 10000\n",
    "X_val = X_train[:val_samples]\n",
    "y_val = y_train[:val_samples]\n",
    "X_train = X_train[val_samples:]\n",
    "y_train = y_train[val_samples:]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314ffae",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Our model architecture combines multiple techniques for effective sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b60b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_words=10000, maxlen=200):\n",
    "    \"\"\"Creates a hybrid CNN-LSTM model for sentiment classification.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # Input and Embedding layer\n",
    "        tf.keras.layers.Embedding(num_words, 128, input_length=maxlen),\n",
    "        \n",
    "        # 1D Convolutional layer for feature extraction\n",
    "        tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        \n",
    "        # LSTM layers for sequential pattern learning\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        \n",
    "        # Dense layers with dropout\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "model = create_model()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954c387",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We'll train the model with early stopping and learning rate reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=2\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72968bef",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93266456",
   "metadata": {},
   "source": [
    "## Model Evaluation and Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fea21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on test samples\n",
    "print(\"\\nSample Predictions vs Actual Labels:\")\n",
    "predictions = model.predict(X_test[:5])\n",
    "for pred, actual in zip(predictions, y_test[:5]):\n",
    "    sentiment = \"Positive\" if pred > 0.5 else \"Negative\"\n",
    "    actual_sentiment = \"Positive\" if actual == 1 else \"Negative\"\n",
    "    confidence = pred if pred > 0.5 else 1 - pred\n",
    "    print(f\"Predicted: {sentiment} (confidence: {confidence[0]:.2f}), Actual: {actual_sentiment}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3aa36",
   "metadata": {},
   "source": [
    "## Try Your Own Review\n",
    "\n",
    "Let's create a function to predict sentiment for any custom review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ff76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(review_text):\n",
    "    # Load the word index\n",
    "    word_index = imdb.get_word_index()\n",
    "    \n",
    "    # Preprocess the review\n",
    "    review_text = review_text.lower()\n",
    "    words = review_text.split()\n",
    "    sequence = [word_index.get(word, 0) for word in words]\n",
    "    sequence = pad_sequences([sequence], maxlen=200)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(sequence)[0][0]\n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Try with a sample review\n",
    "sample_review = \"This movie was fantastic! The acting was great and the story kept me engaged throughout.\"\n",
    "sentiment, confidence = predict_review(sample_review)\n",
    "print(f\"Review: {sample_review}\")\n",
    "print(f\"Prediction: {sentiment} (confidence: {confidence:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
